[
["index.html", "Biostatistics in R Chapter 1 Prerequisites 1.1 Install R and RStudio 1.2 R Packages 1.3 Background Reading", " Biostatistics in R Sahir Rai Bhatnagar 2016-04-15   Chapter 1 Prerequisites  1.1 Install R and RStudio All examples in this book are run in an R environment. You also need a recent version of RStudio, which is a software application that facilitates how you interact with R. It is developed by data enthusiasts who consider statistics to be more than just simulations, formulas and proofs. RStudio emphasizes the following:  Version control: Why I should use version control especially for the solo data analyst. Reproducible research: seamless integration with RMarkdown for creating dynamic documents and presentations Creating R Packages: seamless integration with the devtools package for creating software that implements your statistical method or analysis.    1.2 R Packages The following packages will be called upon at some point, so please install them before getting started with the tutorials. Enter the following command in R: install.packages(c(&quot;knitr&quot;, &quot;data.table&quot;, &quot;rmarkdown&quot;, &quot;dplyr&quot;, &quot;purrr&quot;, &quot;tidyr&quot;,      &quot;ggplot2&quot;, &quot;pwr&quot;, &quot;glmnet&quot;, &quot;boot&quot;, &quot;DAAG&quot;))   1.3 Background Reading The greatest thing about R is that there are so many people out there willing to help you. R users are constantly writing tutorials and creating packages to make your analysis tasks easier. Here is a very targeted list that I suggest reading prior to starting the tutorials  Writing Functions for loops apply vs. for    "],
["intro.html", "Chapter 2 For Loops", " Chapter 2 For Loops You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, 0.1, 0.1)) plot(pressure, type = &quot;b&quot;, pch = 19)    Figure 2.1: Here is a nice figure!   Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable(   head(iris, 20), caption = &#39;Here is a nice table!&#39;,   booktabs = TRUE )  Table 2.1: Here is a nice table!   Sepal.Length Sepal.Width Petal.Length Petal.Width Species     5.1 3.5 1.4 0.2 setosa   4.9 3.0 1.4 0.2 setosa   4.7 3.2 1.3 0.2 setosa   4.6 3.1 1.5 0.2 setosa   5.0 3.6 1.4 0.2 setosa   5.4 3.9 1.7 0.4 setosa   4.6 3.4 1.4 0.3 setosa   5.0 3.4 1.5 0.2 setosa   4.4 2.9 1.4 0.2 setosa   4.9 3.1 1.5 0.1 setosa   5.4 3.7 1.5 0.2 setosa   4.8 3.4 1.6 0.2 setosa   4.8 3.0 1.4 0.1 setosa   4.3 3.0 1.1 0.1 setosa   5.8 4.0 1.2 0.2 setosa   5.7 4.4 1.5 0.4 setosa   5.4 3.9 1.3 0.4 setosa   5.1 3.5 1.4 0.3 setosa   5.7 3.8 1.7 0.3 setosa   5.1 3.8 1.5 0.3 setosa    You can write citations, too. For example, we are using the bookdown package (Xie 2016) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). ddd aaa (Davison and Hinkley 1997) library(boot)  mouse.c &lt;- scan(&quot;http://www.rohan.sdsu.edu/~babailey/stat672/mouse.c.data&quot;) mouse.t &lt;- scan(&quot;http://www.rohan.sdsu.edu/~babailey/stat672/mouse.t.data&quot;)   y &lt;- mouse.c sd(y) ## [1] 42.41691 n.c &lt;- length(mouse.c)  B &lt;- 1e+05  thetahat.c &lt;- mean(mouse.c) sehat.c &lt;- sd(mouse.c) * sqrt((n.c - 1)/n.c^2)  theta.boot &lt;- rep(0, B)  Zb &lt;- rep(0, B) Zbt &lt;- rep(0, B) for (b in 1:B) {          yboot &lt;- sample(mouse.c, size = n.c, rep = T)     thetahat.boot &lt;- mean(yboot)     sehat.boot &lt;- sd(yboot) * sqrt((n.c - 1)/n.c^2)          Zb[b] &lt;- (thetahat.boot - thetahat.c)/sehat.c          Zbt[b] &lt;- (thetahat.boot - thetahat.c)/sehat.boot          theta.boot[b] &lt;- thetahat.boot      } hist(theta.boot, xlab = expression(theta), col = &quot;gray&quot;, main = expression(paste(&quot;Bootstrap resamples of &quot;,      hat(theta)))) box() abline(v = thetahat.c, col = &quot;red&quot;, lwd = 2)  abline(v = quantile(theta.boot, prob = c(0.025, 0.975)), col = &quot;red&quot;, lty = 2,      lwd = 2)  alpha &lt;- 0.05 qs &lt;- quantile(Zb, prob = c(1 - alpha/2, alpha/2)) qst &lt;- quantile(Zbt, prob = c(1 - alpha/2, alpha/2)) abline(v = thetahat.c - qst * sehat.c, col = &quot;green&quot;, lty = 2, lwd = 2) abline(v = thetahat.c - qs * sehat.c, col = &quot;blue&quot;, lty = 2, lwd = 2)   "],
["simulations.html", "Chapter 3 Simulations 3.1 What is a simulation study? 3.2 Why conduct a simulation study?", " Chapter 3 Simulations In this chapter you’ll learn how to conduct a simulation study. The word simulation gets thrown around alot as a trivial concept, though I have found that this term is often either misused by epidemiologists or abused by statisticians. A source of this confusion arises from the fact that it is not well defined. This is not by accident. A simulation study has so many moving parts that it would be impossible to characterize in a single word. However, there are some common underlying themes behind simulation studies. For this reason, the main focus here will be on creating a general roadmap for conducting these types of analyses, that can be applied to your specific setting.  3.1 What is a simulation study? Here are some results from a Google Search:  A numerical technique for conducting experiments on the computer (Davidian 2005) Simulations provide a powerful technique for answering a broad set of methodological and theoretical questions and provide a flexible framework to answer specific questions relevant to one’s own research(Hallgren 2013)  I prefer the second definition because it speaks to the generality of simulations. I see simulations in the following way1:  {\"x\":{\"diagram\":\"\\ngraph LR\\nA((Input))-->B{Procedure}\\nB-->C[Output]\\n\"},\"evals\":[],\"jsHooks\":[]} There are three crucial components to any simulation study:  Procedure - something that doesn’t change throughout the simulation study Input: several values of the same quantity that are passed to the procedure. Output: result from the procedure being applied to the inputs. Should be equal in length to the number of outputs. The following toy example illustrates this point:  f_sample_median &lt;- function(true_mean) {     x &lt;- rnorm(20, mean = true_mean, sd = 1)     median(x) }  (means &lt;- c(1, 2, 3)) ## [1] 1 2 3 (sample_medians &lt;- sapply(means, f_sample_median)) ## [1] 1.119985 1.860057 2.964291 So here, f_sample_median is the procedure that generates 20 random numbers from a normal distribution with standard deviation 1 and a user specified mean, and outputs the sample median. means is the input vector of user specified means, and sample_medians is the output:  {\"x\":{\"diagram\":\"\\ngraph LR\\nA((1))-->B{f_sample_median}\\nD((2))-->B\\nE((3))-->B\\nB-->C[1.12]\\nB-->F[1.86]\\nB-->G[2.96]\\n\"},\"evals\":[],\"jsHooks\":[]} Using this very general definition of the word simulation, I see the following analysis techinques as a type of simulation.  {\"x\":{\"diagram\":\"\\ngraph TB\\nsq[Simulation] --> ci[Bootstrap]\\nsq --> A[Cross Validation]\\nsq --> B[Sensitivity Analysis]\\nsq --> C[Permutations]\\nclassDef green fill:#9f6,stroke:#333,stroke-width:2px;\\nclassDef orange fill:#f96,stroke:#333,stroke-width:4px;\\nclass sq green\\n\"},\"evals\":[],\"jsHooks\":[]} Boostrap, Cross Validation, Sensitivity analysis and Permutations are all similar in that they have multiple inputs with an equal number of outputs, with a fixed procedure. What differentiates them is the context in which they can be applied. The technique you choose depends entirely on the question you’re trying to answer.   3.2 Why conduct a simulation study? The most common applications of simulation studies are the following  3.2.1 Understanding the behaviour of a statistical quantity  3.2.1.1 Example 1: The Central Limit Theorem The Central Limit Theorem is one of the most famous theorems in statistics. This theorem basically states that the sum or mean of independent random variables (with finite mean and variance) will converge to a normal distribution, regardless of the distribution of the random variables. The following figure clearly illustrates this point (Joseph 2010)  In this example, the statistical quantity of interest is the sum of the random variables. We want to verify that the sum of the time it takes to get to Purvis hall follows a normal distribution as the number of observations approaches infinity. Of course the CLT can be proven mathematically, but we can also simulate random numbers from different distributions in R to numerically verify this result. The simulation framework can be seen as follows:  {\"x\":{\"diagram\":\"\\ngraph LR\\nA((Sample size n = 20))-->B{Generate n random numbers fromdifferent distributions}\\nD((Sample size n = 100))-->B\\nE((Sample size n = 1000))-->B\\nB-->C[Mean and variance ofsum based on n = 20]\\nB-->F[Mean and variance ofsum based on n = 100]\\nB-->G[Mean and variance ofsum based on n = 1000]\\n\"},\"evals\":[],\"jsHooks\":[]} Here we show a single run of the simulation with \\(n = 10,000\\) # number of observations to generate from each distribution n &lt;- 10000  # true parameters of each distribution mean.walk &lt;- 4 var.walk &lt;- 1 mean.bus &lt;- (4 + 16)/2 var.bus &lt;- (16 - 4)^2/12 mean.ride &lt;- 8 var.ride &lt;- 8 mean.climb &lt;- 6 * 0.5 var.climb &lt;- 6 * 0.5^2 mean.fall &lt;- 1/4 var.fall &lt;- 1/4^2  # generate random samples from each distribution walk &lt;- rnorm(n, mean.walk, var.walk) bus &lt;- runif(n, 4, 16) ride &lt;- rpois(n, mean.ride) climb &lt;- rgamma(n, shape = 6, scale = 0.5) fall &lt;- rexp(n, rate = 1/mean.fall)  # true mean and variance of sum mean.sum &lt;- mean.walk + mean.bus + mean.ride + mean.climb + mean.fall var.sum &lt;- var.walk + var.bus + var.ride + var.climb + var.fall  # create a data frame of the data and calculate the sum for each sample DT &lt;- data.frame(walk, bus, ride, climb, fall) DT$sum &lt;- apply(DT, 1, sum)  # plot each histogram and superimpose theoretical distribution to empirical # one par(mfrow = c(2, 3)) hist(walk, main = &quot;Normal(mean=4,sd=1)&quot;, freq = FALSE) hist(bus, main = &quot;Uniform(a=4,b=16)&quot;, freq = FALSE) hist(ride, main = &quot;Poisson(lambda=8)&quot;, freq = FALSE) hist(climb, main = &quot;Gamma(shape=6, scale=0.5)&quot;, freq = FALSE) hist(fall, main = &quot;Exponential(rate=4)&quot;, freq = FALSE) hist(DT$sum, xlab = &quot;Total time&quot;, main = paste(&quot;Distribution of Sum, n = &quot;,      n), freq = FALSE) curve(dnorm(x, mean = mean.sum, sd = sqrt(var.sum)), 0, 50, add = TRUE, lwd = 2,      col = &quot;red&quot;)    3.2.1.2 Example 2: Power Calculation When performing Student’s t-test to compare difference in means between two groups, it is a useful exercise to determine the effect of unequal sample sizes in the comparison group on power. Large imbalances generally will not have adequate statistical power to detect even large effect sizes associated with a factor, leading to a high Type II error rate. To jusity this reasoning I performed a power analysis for different group sizes. A power curve typically has power (the probability of rejecting the null hypothesis when the alternative hypothesis is true) on the y-axis, and a measure of association (or effect size) on the x-axis. For a given sample size of both groups, we can calculate a power curve by varying the true effect size. The simulation framework can be seen as follows:  {\"x\":{\"diagram\":\"\\ngraph LR\\nA((Effect Size 0.1))-->B{Procedure to calculatepower for t-test}\\nD((Effect Size 0.5))-->B\\nE((Effect Size 1))-->B\\nB-->C[Power for effect size of 0.1]\\nB-->F[Power for effect size of 0.5]\\nB-->G[Power for effect size of 1]\\n\"},\"evals\":[],\"jsHooks\":[]} I considered the following group sizes:  n1 = 28, n2 = 1406: n1 represents 2% of the entire sample size of 1434 n1 = 144, n2 = 1290: n1 represents 10% of the entire sample size of 1434 n1 = 287, n2 = 1147: n1 represents 20% of the entire sample size of 1434 n1 = 430, n2 = 1004: n1 represents 30% of the entire sample size of 1434 n1 = 574, n2 = 860: n1 represents 40% of the entire sample size of 1434 n1 = 717, n2 = 717: equal size groups (this is optimal because it leads to the highest power for a given effect size)  In the figure below we plot the power curves for the \\(t\\)-test, as a function of the effect size, assuming a Type I error rate of 5%.  Here is the code used to produce the above plot library(pwr) # for power calcs library(dplyr) # for data manipulation library(tidyr) # for data manipulation library(ggplot2) # for plotting power curves  # Generate power calculations ptab &lt;- cbind(NULL, NULL)         for (i in seq(0,1, length.out = 200)){   pwrt1 &lt;- pwr.t2n.test(n1 = 28, n2 = 1406,                          sig.level = 0.05, power = NULL,                          d = i, alternative=&quot;two.sided&quot;)   pwrt2 &lt;- pwr.t2n.test(n1 = 144, n2 = 1290,                          sig.level = 0.05, power = NULL,                          d = i, alternative=&quot;two.sided&quot;)   pwrt3 &lt;- pwr.t2n.test(n1 = 287, n2 = 1147,                          sig.level = 0.05, power = NULL,                          d = i, alternative=&quot;two.sided&quot;)   pwrt4 &lt;- pwr.t2n.test(n1 = 430, n2 = 1004,                          sig.level = 0.05, power = NULL,                          d = i, alternative=&quot;two.sided&quot;)   pwrt5 &lt;- pwr.t2n.test(n1 = 574, n2 = 860,                          sig.level = 0.05, power = NULL,                          d = i, alternative=&quot;two.sided&quot;)   pwrt6 &lt;- pwr.t2n.test(n1 = 717, n2 = 717,                          sig.level = 0.05, power = NULL,                          d = i, alternative=&quot;two.sided&quot;)   ptab &lt;- rbind(ptab, cbind(pwrt1$d, pwrt1$power,                             pwrt2$d, pwrt2$power,                             pwrt3$d, pwrt3$power,                             pwrt4$d, pwrt4$power,                             pwrt5$d, pwrt5$power,                             pwrt6$d, pwrt6$power)) }  ptab &lt;- cbind(seq_len(nrow(ptab)), ptab)  colnames(ptab) &lt;- c(&quot;id&quot;,&quot;n1=28, n2=1406.effect size&quot;,&quot;n1=28, n2=1406.power&quot;,                     &quot;n1=144, n2=1290.effect size&quot;,&quot;n1=144, n2=1290.power&quot;,                     &quot;n1=287, n2=1147.effect size&quot;,&quot;n1=287, n2=1147.power&quot;,                     &quot;n1=430, n2=1004.effect size&quot;,&quot;n1=430, n2=1004.power&quot;,                     &quot;n1=574, n2=860.effect size&quot;,&quot;n1=574, n2=860.power&quot;,                     &quot;n1=717, n2=717.effect size&quot;,&quot;n1=717, n2=717.power&quot;)  # get data into right format for ggplot2 temp &lt;- ptab %&gt;%   as.data.frame() %&gt;%   gather(key = name, value = val, 2:13) %&gt;%   separate(col = name, into = c(&quot;group&quot;, &quot;var&quot;), sep = &quot;\\\\.&quot;) %&gt;%   spread(key = var, value = val)  # factor group temp$group &lt;- factor(temp$group,                  levels = c(&quot;n1=28, n2=1406&quot;, &quot;n1=144, n2=1290&quot;,                  &quot;n1=287, n2=1147&quot;, &quot;n1=430, n2=1004&quot;,                 &quot;n1=574, n2=860&quot;, &quot;n1=717, n2=717&quot;))   # plot p &lt;- ggplot(temp, aes(x = `effect size`, y = power, color = group)) p +  geom_line(size=2) +  theme_bw() +  theme(legend.position = &quot;bottom&quot;,        axis.text=element_text(size=14),        axis.title=element_text(size=14),        legend.text=element_text(size=14))    3.2.2 Model selection and assessment Model selection refers to  Estimating the performance of different models in order to choose the best one (Hastie, Tibshirani, and Friedman 2009)  Model assessment refers to  Estimating the prediction error on new data, given that a model has been selected  In real data analysis problems we often have many models to choose from, for example, different combinations of covariates result in different models. The question is how can we can select the model that best fits the data and generalizes to other datasets? Information criteria such as the AIC, BIC and QIC can be used in different settings, however there are some known pitfalls to these (see for example Y. Wang et al. (2015)). Here is an example of model selection in the penalized regression setting. We consider a regression model for an outcome variable \\(Y=(y_1, \\ldots, y_n)\\) which follows an exponential family. Let \\(\\boldsymbol{X} = (X_{1}, \\ldots, X_{p})\\) be a matrix of covariates, where \\(p &gt;&gt; n\\). Consider the regression model with main effects: \\begin{align} g(\\boldsymbol{\\mu})  = &amp; \\beta_0  + \\beta_1 X_{1} + \\cdots + \\beta_p X_p  \\label{eq:linpred1} \\end{align} where \\(g(\\cdot)\\) is a known link function and \\(\\boldsymbol{\\mu} = \\mathsf{E}\\left[Y|\\boldsymbol{X}, \\boldsymbol{\\beta} \\right]\\). Our goal is to estimate the parameters \\(\\boldsymbol{\\beta} = \\left(\\beta_1, \\beta_2, \\ldots, \\beta_p\\right) \\in \\mathbb{R}^{p+1}\\). Due to the large number of parameters to estimate with respect to the number of observations, one commonly-used approach is to shrink the regression coefficients by placing a constraint on the values of \\(\\boldsymbol{\\beta}\\). For example, the LASSO (R. Tibshirani 1996) penalizes the squared loss of the data with the  of the regression coefficients resulting in a method that performs both model selection and estimation: \\begin{equation} argmin_{\\beta_0, \\boldsymbol{\\beta} } \\,\\,\\, \\frac{1}{2} \\left\\Vert Y - g(\\boldsymbol{\\mu}) \\right\\Vert ^2 + \\lambda  \\left\\Vert \\boldsymbol{\\beta} \\right\\Vert_1  \\label{eq:lassolikelihood} \\end{equation} where \\(\\left\\Vert Y - g(\\boldsymbol{\\mu})\\right\\Vert^2 = \\sum_i (y_i - g(\\mu_i))^2\\), \\(\\left\\Vert \\boldsymbol{\\beta}\\right\\Vert_1 = \\sum_j | \\beta_j |\\) and \\(\\lambda \\geq 0\\) is a data driven tuning parameter that can set some of the coefficients to zero when sufficiently large. It should be noted here that different tuning parameters will lead to different models. So the question here is how to choose the optimal \\(\\lambda\\). A very common techinque is to use cross-validation (Hastie, Tibshirani, and Friedman 2009):  The data is split into \\(K = 5\\) (or 10) roughly equal sized parts. The model is fit on all the data except the Kth part, and the prediction error of the fitted model is calculated on the Kth part. This is repeated K times, and the error from each fold is averaged. The simulation framework can be seen as follows:  {\"x\":{\"diagram\":\"\\ngraph LR\\nA((Fold 1))-->B{Fit LASSO Modelon remaining folds}\\nD((Fold 2))-->B\\nE((Fold 3))-->B\\nF((Fold 4))-->B\\nG((Fold 5))-->B\\nB-->C[Prediction erroron left-out fold]\\nB-->H[Prediction erroron left-out fold]\\nB-->I[Prediction erroron left-out fold]\\nB-->J[Prediction erroron left-out fold]\\nB-->K[Prediction erroron left-out fold]\\n\"},\"evals\":[],\"jsHooks\":[]} Note this is for a single tuning parameter. We would repeat the above steps for a range of tuning parameters. This technique for choosing the tuning parameter in Equation \\ref{eq:lassolikelihood} is implemented in the glmnet package (J. Friedman et al. 2016): library(glmnet) set.seed(12345)  # number of subjects n &lt;- 100  # number of covariates p &lt;- 1000  # number of true non-zero coefficients nzc &lt;- 25  # covariates x &lt;- matrix(rnorm(n * p), n, p)  # true beta coefficients beta &lt;- rnorm(nzc)  # generate response. only the first nzc are associated with y y &lt;- x[, seq(nzc)] %*% beta + rnorm(n) * 5  # perform cross-validation cvobj &lt;- cv.glmnet(x, y)  # plot cross validation curve plot(cvobj)  # final selected model based on CV-selected lambda coefobj &lt;- coef(cvobj, s = &quot;lambda.min&quot;)  # non-zero elements of selected model and their effect estimate coefobj[glmnet:::nonzeroCoef(coefobj), , drop = F] ## 31 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ##                       1 ## (Intercept)  0.33746196 ## V11         -0.88033906 ## V16         -1.91994340 ## V17          1.11662951 ## V19          0.58131900 ## V39          0.11067061 ## V49         -0.10673164 ## V54         -0.09201722 ## V143        -0.17211302 ## V204         0.08452316 ## V234         0.16363999 ## V271         0.09728157 ## V275         0.12263144 ## V312        -0.08838243 ## V338        -0.34694844 ## V339         0.17656390 ## V388        -0.00927594 ## V423         0.24150449 ## V429        -0.03885577 ## V493        -0.10920236 ## V541         0.37624248 ## V548         1.07802523 ## V618         0.12164737 ## V660        -0.15121156 ## V702        -0.19438313 ## V790         0.43667721 ## V867         0.25084326 ## V960         0.23430632 ## V981        -0.31318982 ## V984         0.51441461 ## V985         0.30346286 While we have shown an example of model selection, cross validation can also be used to assess the chosen model but this should be done on an independent data set, i.e., one that was not used to train and select the model.   3.2.3 Calculating measures of uncertainty The Bootstrap (Efron and Tibshirani 1994) is a widely used procedure to calculate measures of uncertainty (e.g. confidence intervals, standard errors of paramters). Its popularity is due to its relative simplicity. Furthermore, the data analyst often has no choice but to bootstrap because no closed form mathematical solution exists. The main idea is summarised in the following diagram:   3.2.3.1 Example 1: Non-parametric Bootstrap for Confidence Intervals of the mean2 The following data are the survival times for mice in a randomized experiment comparing a treated group with a control group. # control group (mouse.c &lt;- scan(&quot;http://www.rohan.sdsu.edu/~babailey/stat672/mouse.c.data&quot;)) ## [1]  52 104 146  10  50  31  40  27  46 # treatment group (mouse.t &lt;- scan(&quot;http://www.rohan.sdsu.edu/~babailey/stat672/mouse.t.data&quot;)) ## [1]  94 197  16  38  99 141  23 We are interested in bootstrap confidence intervals for the mean survival time in the control group: # number of mice in control group n.c &lt;- length(mouse.c)  # point estimates of mean and standard error of survival time mean(mouse.c) ## [1] 56.22222 # recall that standard error of the mean is sigma/sqrt{n} if the presumption # of normality is made than this is the plug-in estimate of the standard # error sd(mouse.c) * sqrt((n.c - 1)/n.c^2) ## [1] 13.33035 We take advantage of thee very flexible boot package (Canty and Ripley 2016): library(boot)  # define a function where the first argument is the data # and the second argument will take a vector of indices used to subset the data meancalc &lt;- function(x,i) {        # always subset the data first      d &lt;- x[i]      m &lt;- mean(d)      n &lt;- length(i)      v &lt;- (n-1)*var(d)/n^2            # bootstrap variances needed for studentized intervals      return(c(m, v)) }  # run the boot function for B=10,000 bootstrap samples # the bootstrap statistics returns the bias for the sample mean (t1) # and sample variance  (bmean &lt;- boot(mouse.c, meancalc, R = 1e4)) ##  ## ORDINARY NONPARAMETRIC BOOTSTRAP ##  ##  ## Call: ## boot(data = mouse.c, statistic = meancalc, R = 10000) ##  ##  ## Bootstrap Statistics : ##      original      bias    std. error ## t1*  56.22222  -0.1363111    13.37626 ## t2* 177.69822 -19.7009962    83.86027 # calculate different types of confidence intervals mouse.ci &lt;- boot.ci(bmean)  # histogram of bootstrap replicates and confidence intervals hist(bmean$t[,1], xlab = expression(theta), col=&#39;gray&#39;, xlim = c(0,150),      main = expression(paste(&#39;Bootstrap resamples of &#39;, hat(theta))));box()   abline(v = mean(mouse.c), col=&#39;red&#39;, lwd=2) abline(v=mouse.ci$normal[2:3],col=&#39;green&#39;) abline(v=mouse.ci$basic[4:5],col=&#39;blue&#39;) abline(v=mouse.ci$student[4:5],col=&#39;green&#39;,lty=2) abline(v=mouse.ci$percent[4:5],col=&#39;blue&#39;,lty=2) abline(v=mouse.ci$bca[4:5],col=&#39;orange&#39;)  legend(&quot;topright&quot;,c(&#39;Est.&#39;,&#39;Normal&#39;,&#39;Basic&#39;,&#39;Student&#39;,&#39;Percent&#39;,&#39;BCa&#39;), col=c(&#39;red&#39;,&#39;green&#39;,&#39;blue&#39;,&#39;green&#39;,&#39;blue&#39;,&#39;orange&#39;),lty=c(1,1,1,2,2,1))    3.2.3.2 Example 2: Non-parametric Bootstrap for Causal Effect Estimates in Marginal Structural Models Labour Training Evaluation Data: This data frame from the DAAG package (Maindonald and Braun 2015) contains 445 rows and 10 columns. These data are from an investigation of the effect of training on changes, between 1974-1975 and 1978, in the earnings of individuals who had experienced employment difficulties. Data are for the male experimental control and treatment groups. The goal of this study was to determine if a job training intervention led to increased earnings. The training program was created to encourage people to get to work and to take better jobs. Here are the data:  {\"x\":{\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"1100\",\"261\",\"310\",\"410\",\"510\",\"610\",\"710\",\"810\",\"910\",\"1010\",\"1110\",\"1210\",\"1310\",\"1410\",\"1510\",\"1610\",\"1710\",\"1810\",\"1910\",\"2010\",\"2110\",\"2210\",\"2310\",\"2410\",\"2510\",\"262\",\"271\",\"281\",\"291\",\"301\",\"311\",\"321\",\"331\",\"341\",\"351\",\"361\",\"371\",\"381\",\"391\",\"401\",\"411\",\"421\",\"431\",\"441\",\"451\",\"461\",\"471\",\"481\",\"491\",\"501\",\"511\",\"521\",\"531\",\"541\",\"551\",\"561\",\"571\",\"581\",\"591\",\"601\",\"611\",\"621\",\"631\",\"641\",\"651\",\"661\",\"671\",\"681\",\"691\",\"701\",\"711\",\"721\",\"731\",\"741\",\"751\",\"761\",\"771\",\"781\",\"791\",\"801\",\"811\",\"821\",\"831\",\"841\",\"851\",\"861\",\"871\",\"881\",\"891\",\"901\",\"911\",\"921\",\"931\",\"941\",\"951\",\"961\",\"971\",\"981\",\"991\",\"1001\",\"1011\",\"1021\",\"1031\",\"1041\",\"1051\",\"1061\",\"1071\",\"1081\",\"1091\",\"1101\",\"1111\",\"1121\",\"1131\",\"1141\",\"1151\",\"1161\",\"1171\",\"1181\",\"1191\",\"1201\",\"1211\",\"1221\",\"1231\",\"1241\",\"1251\",\"1261\",\"1271\",\"1281\",\"1291\",\"1301\",\"1311\",\"1321\",\"1331\",\"1341\",\"1351\",\"1361\",\"1371\",\"1381\",\"1391\",\"1401\",\"1411\",\"1421\",\"1431\",\"1441\",\"1451\",\"1461\",\"1471\",\"1481\",\"1491\",\"1501\",\"1511\",\"1521\",\"1531\",\"1541\",\"1551\",\"1561\",\"1571\",\"1581\",\"1591\",\"1601\",\"1611\",\"1621\",\"1631\",\"1641\",\"1651\",\"1661\",\"1671\",\"1681\",\"1691\",\"1701\",\"1711\",\"1721\",\"1731\",\"1741\",\"1751\",\"1761\",\"1771\",\"1781\",\"1791\",\"1801\",\"1811\",\"1821\",\"1831\",\"1841\",\"1851\"],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[23,26,22,18,45,18,24,34,24,36,21,28,27,19,20,34,24,22,25,39,19,44,27,25,31,34,21,33,18,26,31,35,20,25,25,35,20,25,27,20,26,38,34,19,32,20,23,38,24,23,20,21,25,22,23,24,29,24,22,28,18,26,25,24,26,36,22,25,27,29,24,22,24,29,25,30,22,55,20,34,22,32,31,18,50,25,23,38,25,42,39,34,24,32,27,26,44,25,25,28,32,22,19,31,23,33,27,29,23,25,25,24,28,26,30,25,29,25,28,23,54,33,20,45,39,26,23,27,33,25,23,18,17,19,18,18,17,19,19,18,18,18,17,18,18,19,18,17,17,19,17,18,18,17,19,19,17,20,17,17,17,19,19,20,18,18,17,19,17,17,18,21,18,19,24,28,25,21,39,36,24,17,18,18,28,27,31,22,31,26,18,23,20,19,18,17,27,27,28,28,21,17,26,29,17,22,24,20,18,24,21,30,31,17,19,23,22,29,22,29,19,17,18,19,20,33,36,25,19,23,17,43,26,27,19,28,28,26,31,23,20,28,39,21,22,20,21,23,29,28,30,25,28,22,44,21,28,29,25,22,37,22,30,27,33,22,23,32,22,33,19,21,18,27,17,19,27,23,40,26,23,41,38,24,18,29,25,27,17,24,17,48,25,20,25,42,25,23,46,24,21,19,17,18,20,25,17,17,25,23,28,31,18,25,30,17,37,41,42,22,17,29,35,27,29,28,27,23,45,29,27,46,18,25,28,25,22,21,40,22,25,18,38,27,27,38,23,26,21,25,31,17,25,21,44,25,18,42,25,31,24,26,25,18,19,43,27,17,30,26,20,17,20,18,27,21,27,20,19,23,29,18,19,27,18,27,22,23,23,20,17,28,26,20,24,31,23,18,29,26,24,25,24,46,31,19,19,27,26,20,28,24,19,23,42,25,18,21,27,21,22,31,24,29,29,19,19,31,22,21,17,26,20,19,26,28,22,33,22,29,33,25,35,35,33],[10,12,9,9,11,9,8,11,4,10,14,9,7,11,8,12,10,8,11,9,9,9,8,8,10,10,7,12,10,12,12,10,12,11,10,11,10,9,10,11,11,8,10,12,8,9,10,10,11,11,7,11,10,11,11,12,11,11,9,11,10,10,10,10,5,10,11,12,11,8,12,10,7,12,11,12,8,3,10,11,12,12,10,9,10,11,10,10,10,10,12,13,7,11,13,10,11,11,12,12,10,10,9,10,11,11,10,11,10,9,10,10,8,6,14,10,11,12,13,11,11,5,8,9,6,12,10,12,9,10,8,8,8,9,8,11,11,10,10,9,9,10,10,7,11,10,9,10,10,11,8,10,9,8,6,10,11,9,9,10,9,11,10,9,9,11,10,11,10,9,10,9,10,11,9,11,11,7,11,12,12,11,10,10,10,13,12,9,10,10,9,11,12,10,11,10,12,11,12,11,10,10,11,9,10,11,11,12,12,11,9,8,11,10,9,11,12,12,10,13,11,9,10,12,6,11,11,11,11,8,11,10,11,11,11,11,12,8,12,11,12,10,12,8,11,11,11,12,9,9,11,10,11,10,9,9,11,9,9,10,11,9,12,11,8,9,12,11,16,12,9,13,8,10,7,10,13,10,12,12,11,14,9,11,10,11,11,10,10,11,10,4,11,12,12,14,5,12,8,10,12,9,8,8,11,11,8,9,5,12,8,11,11,12,11,10,9,4,14,11,8,8,10,11,4,9,11,7,5,13,9,13,6,12,15,11,12,9,11,11,12,12,12,13,8,11,8,11,12,8,11,10,11,12,11,12,9,12,10,9,10,10,11,11,11,9,13,9,11,10,9,9,12,11,12,12,12,12,10,12,14,10,9,13,11,9,12,10,12,11,9,11,11,11,11,9,8,10,12,11,9,12,10,8,12,11,8,11,11,10,10,12,8,12,9,13,9,12,10,8,9,4,10,10,12,10,11,9,10,9,10,12,9,10,10,11,12,11,12,10,12,14,9,8,11],[1,0,1,1,1,1,0,1,0,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,0,1,0,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,1,0,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,0,1,0,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1,0,1,0,1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,0,0,1,1,1,0,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,1],[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,1,0,1,0,1,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,0,1,0,0,0,1,1,1,1,1],[1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,0,1,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,1,1,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,0,1,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,0,0,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,0,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,1,0,0,1,0,1,1,1,1,0,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,0,1,0,0,1,0,1,1,1,0,0,0,0,1,1,1,1,0,1,1,1,1,0,1,0,1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,0,0,0,0,1,0,0,1,1,0,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,1,1,1,0,1,0,1,0,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1,1,0,1,0,1,0,0,1,1,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,591.4991,1563.249,1626.623,2788.498,3472.948,5281.245,33799.95,0,0,0,989.2678,960.4268,0,1471.293,5214.306,0,0,0,6140.367,0,6382.309,0,0,1064.699,0,0,3065.194,0,2431.949,6661.063,4905.119,4699.963,0,1203.609,7914.131,0,557.6988,0,2669.725,2988.412,0,17711.88,1442.681,8409.627,0,4380.017,22859.44,0,718.2492,721.3406,0,1716.509,8417,6006.879,10523.83,5443.734,15209.99,3504.014,7724.283,4080.73,2502.868,0,0,6337.492,8593.163,10585.13,1126.29,0,7617.362,7182.492,8293.345,19785.32,39570.68,8810.067,8009.164,2992.534,5721.696,9268.943,10222.41,0,13519.97,824.3889,27864.36,12260.78,31886.43,17491.45,9594.308,24731.62,25720.92,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2027.999,6083.994,445.1704,989.2678,858.2543,3670.872,3670.872,2143.413,0,0,5506.308,0,0,0,9381.566,3678.231,0,5605.852,0,9385.74,3637.498,1716.509,0,0,16318.62,824.3886,0,0,2143.411,10881.94,0,9154.7,14426.79,4250.402,3165.658,0,2305.026,0,2206.94,0,5005.731,0,13765.75,2636.353,6269.341,0,12362.93,0,6473.683,1001.146,989.2678,2192.877,8517.589,11703.2,0,9748.387,0,5424.485,10717.03,1468.348,6416.47,1291.468,8408.762,12260.78,4121.949,25929.68,1929.029,492.2305,0,6759.994,0,20279.95,35040.07,13602.43,13732.07,14660.71],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,83.68964,142.3973,159.8847,165.2077,240.1067,273.5525,367.8234,474.5018,494.643,506.4076,520.4463,558.7734,559.596,580.7901,591.8151,604.1539,645.2723,664.569,752.3901,766.2986,803.343,863.4795,1162.362,1168.902,1174.991,1203.824,1239.628,1321.66,1327.993,1371.473,1405.512,1468.383,1577.167,1706.657,1726.445,1734.559,1778.089,1896.023,2003.68,2080.209,2174.955,2265.579,2445.589,2595.267,2682.125,2814.195,2850.61,2899.82,3063.878,3072.726,3285.68,3403.056,3796.029,4128.443,4184.732,4491.884,4503.064,5393.895,5551.456,5562.598,5613.909,5716.407,6004.728,6449.48,6608.137,6608.304,6974.484,7666.875,8920.471,8960.684,9160.693,9210.447,9311.938,9319.444,10033.91,10598.67,10857.24,12357.22,13371.25,16341.16,16946.63,23031.98,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,74.34345,165.2077,214.5636,334.0493,334.0494,357.9499,377.5686,385.2741,501.0741,679.6734,798.9079,798.9079,853.7225,919.5579,934.4454,936.1773,936.4386,1117.439,1220.836,1253.439,1284.079,1392.853,1484.994,1666.113,1698.607,1713.15,1784.274,1817.284,2226.266,2288.675,2409.274,2421.947,2594.723,2611.218,2615.276,2657.057,2666.274,2754.646,2777.355,2836.506,2842.764,2937.264,3039.96,3058.531,3090.732,3287.375,3332.409,3550.075,3695.897,3836.986,4023.211,4078.152,4398.95,4878.937,5324.109,5463.803,5517.841,5588.664,5749.331,5793.852,5794.831,5875.049,6056.754,6788.958,6871.856,7055.702,7867.916,8455.504,8853.674,10941.35,11536.57,13830.64,17976.15,25142.24],[0,12383.68,0,10740.08,11796.47,9227.052,10569.27,6040.335,3880.833,0,5775.062,0,0,0,0,2113.722,7618.639,9920.945,4196.375,0,16658.25,9722.003,3783.66,3515.929,17014.59,0,0,5970.257,1859.167,6191.943,7284.394,445.8309,0,0,7367.04,0,2015.503,15791.13,1135.47,6378.72,7176.187,0,7952.54,0,7152.132,8329.823,0,12429.91,0,5088.76,4374.04,1553.291,0,1698.304,0,11294.63,0,14626.39,12898.38,5767.133,6527.919,3931.238,20942.24,0,0,14690.35,0,3418.097,11197.34,0,0,0,1455.69,1890.937,4485.616,13613.35,1390.509,5843.796,8598.522,2920.199,0,6735.32,0,0,0,44.75546,0,0,3701.812,6930.336,3795.799,5193.247,2193.528,11120.53,7609.518,2169.027,0,1264.232,0,0,0,0,5712.643,0,0,0,1184.882,10225.88,0,4715.371,289.7899,0,8190.421,4813.05,7344.678,0,0,0,0,4350.907,7812.522,0,3644.655,4844.803,0,0,0,14792.9,0,0,3746.701,1568.15,7010.444,3811.683,10798.56,4657.273,8551.533,4309.878,5286.396,12486.17,10877.35,202.2847,2657.705,4132.577,11303.14,0,0,0,2189.426,0,10210.99,11048.08,0,8993.865,5071.801,3194.01,0,5193.089,0,275.5661,3590.702,0,12797.67,2035.914,2389.679,0,8469.275,0,1143.387,5114.814,781.2243,3343.224,9602.439,0,16461.57,6771.622,0,11011.57,0,0,0,4251.127,2891.668,5514.365,4858.895,4812.576,0,604.1987,14527.88,0,7300.498,0,4159.919,0,5497.591,0,0,0,16477.02,0,39483.53,11306.27,6672.021,9378.653,5088.986,2639.29,9495.902,20893.11,0,10361.69,1740.199,0,0,6354.194,7171,5573.547,439.6881,16969.95,5344.021,2725.322,9772.283,0,0,1720.907,0,18859.89,1324.542,284.6584,11195.93,0,0,7565.273,0,0,0,4974.586,12780.02,3523.578,0,10274.84,4779.72,16988.18,499.2572,3083.581,3708.719,7659.218,20857.84,7078.178,0,1239.844,3982.801,0,0,7094.92,12359.31,0,0,16900.3,7343.964,5448.801,9930.046,3595.894,24909.45,7506.146,289.7899,4056.494,0,8472.158,2164.022,12418.07,8173.908,17094.64,0,18739.93,3023.879,3228.503,14581.86,7693.4,10804.32,10747.35,0,5149.501,6408.95,1991.4,11163.17,9642.999,9897.049,11142.87,16218.04,995.7002,0,6551.592,1574.424,0,3191.753,20505.93,6181.88,5911.551,3094.156,0,1254.582,13188.83,8061.485,2787.96,3972.54,0,0,0,12187.41,4843.176,0,8087.487,0,2348.973,590.7818,0,1067.506,7284.986,13167.52,1048.432,0,1923.938,4666.236,549.2984,762.9146,10694.29,0,0,8546.715,7479.656,0,647.2046,0,11965.81,9598.541,18783.35,18678.08,0,23005.6,6456.697,0,2321.107,4941.849,0,0,0,3881.284,17230.96,8048.603,0,14509.93,0,0,9983.784,0,5587.503,4482.845,2456.153,0,26817.6,0,9265.788,485.2298,4814.627,7458.105,0,34099.28,1953.268,0,0,8881.665,6210.67,0,929.8839,0,12558.02,22163.25,1652.637,8124.715,671.3318,17814.98,9737.154,17685.18,0,4321.705,1773.423,0,11233.26,559.4432,1085.44,5445.2,60307.93,1460.36,6943.342,4032.708,10363.27,4232.309,11141.39,0,13385.86,4849.559,0,1660.508,0,2484.549,4146.603,9970.681,0,26372.28,5615.189,3196.571,6167.681,7535.942,8484.239,1294.409,0,5010.342,9371.037,0,4279.613,3462.564,7382.549,0,0,10976.51,13829.62,6788.463,9558.501,13228.28,743.6666,5522.788,1424.944,1358.643,0,672.8773,0,10092.83,6281.433,12590.71,5112.014,15952.6,36646.95,12803.97,3786.628,4181.942]],\"container\":\"\\n  \\n    \\n       \\u003c/th>\\n      trt\\u003c/th>\\n      age\\u003c/th>\\n      educ\\u003c/th>\\n      black\\u003c/th>\\n      hisp\\u003c/th>\\n      marr\\u003c/th>\\n      nodeg\\u003c/th>\\n      re74\\u003c/th>\\n      re75\\u003c/th>\\n      re78\\u003c/th>\\n    \\u003c/tr>\\n  \\u003c/thead>\\n\\u003c/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false},\"callback\":null,\"filter\":\"none\",\"selection\":\"multiple\"},\"evals\":[],\"jsHooks\":[]} We are interested in evaluating the effect of a binary treatment, participation in the training program or not, on the continuous outcome of real income in 1978. In this study, treatment is not randomized, therefore direct comparison of the 1978 earnings between treated and untreated will lead to a biased estimate of the effect of the training program. The covariates were not balanced between groups. The propensity score (PS) for an individual, defined as the conditional probability of being treated given the individual’s covariates, can be used to balance the covariates in the two groups, and thus reduce this bias. Once estimated, the propensity score can be used to reduce bias through weighted regression (dAgostino 1998). Propensity scores provide a summary measure to control for multiple confounders simultaneously. Inverse probability of treatment weighting (IPTW) using PS has several advanages over multiple linear regression adjustment for several reasons. It is simpler to determine whether the propensity score (PS) model has been adequately specified (Austin 2011) by checking if the distribution of baseline measured covariates is similar in both groups conditional on the PS. Regression diagnostics are complicated when several predictors and their interactions are put in the model. Furthermore, for regression based methods, goodness of fit measures provide no test of whether the outcome model has been correctly specified, and the degree to which the fitted model has eliminated systematic difference between control and treatment groups (Austin 2011). The PS is estimated without reference to the outcome whereas this is not the case in regression methods, which can lead to hypothesis generation, i.e. continually fit different models until a desirable result is achieved. Finally, IPTW methods can fit very complicated PS models with interactions and higher order terms, even non-parametrically, without concern of over-fitting since the goal is to obtain the best estimated probability of treatment assignment. Let \\(Z\\) denote the indicator for receiving treatment, \\(\\mathbf{X}\\) a vector of pre-exposure covariates, and \\(Y\\) the observed outcome. To estimate the average causal treatment effect we use the potential outcomes framework (Rosenbaum and Rubin 1983). Let \\(Y_0\\) and \\(Y_1\\) denote the outcome a subject would have if they didn’t and did received the training program, respectively. The causal effect of the training program on earnings may be defined as for instance the difference \\(Y_1-Y_0\\). Since only one of the potential outcomes can be observed, in practice this difference is replaced by the average causal effect \\(\\Delta=E(Y_1)-E(Y_0)\\). If treatment assignment is completely randomized, i.e., \\((Y_0,Y_1)\\perp Z\\) then \\(\\bar{Y}_1 - \\bar{Y}_0\\) is an unbiased estimator for \\(\\Delta\\). However in this study this assumption is violated because the value of \\(Z\\) has not been determined through complete randomization. It may be then that \\((Y_0,Y_1)\\) and \\(Z\\) have common determinants, meaning that the treatment is informative of the pair of potential outcomes. If we assume \\(\\mathbf{X}\\) contains all confounders then among subjects sharing the same \\(\\mathbf{X}\\) there will be no association between exposure and potential outcomes, i.e., \\((Y_0,Y_1)\\perp Z|\\mathbf{X}\\). The propensity score is defined as \\(e(\\mathbf{X},\\mathbf{\\beta})= P(Z=1|\\mathbf{X})\\). We estimate this probability using logistic regression where \\(\\mathbf{X}\\) contains age, age\\(^2\\), years of education and its square term, indicators for no degree, black, hispanic, actual 1974-1975 earnings and their square terms. Then we can estimate the causal parameter \\(\\Delta\\) using a weighted linear regression; regressing 1978 earnings on treatment with weights corresponding to \\(e(\\mathbf{X},\\mathbf{\\widehat{\\beta}})^{-1}\\) for treated and \\(\\left(1-e(\\mathbf{X},\\mathbf{\\widehat{\\beta}})\\right)^{-1}\\) for untreated individuals. To determine the effect of treatment on change in earnings from 1975 to 1978, we will fit an IPTW regression model using 1975 earnings as the response. The covariates used for calculating the PS will be similar to the previous model except it will not have 1975 earnings in it. The desired effect estimate will then be the difference of the causal paramters estimated from each model, i.e., \\(\\widehat{\\Delta}_{\\left(1978\\right)} - \\widehat{\\Delta}_{\\left(1975\\right)}\\). We will repeat this calculation on \\(B=4000\\) (stratified on treatment) bootstrap samples to get an estimate of the standard error of this statistic. library(DAAG) # for the data ## Loading required package: lattice ##  ## Attaching package: &#39;lattice&#39; ## The following object is masked from &#39;package:boot&#39;: ##  ##     melanoma library(boot) # for bootstrap confidence intervals  did &lt;- function(data, indices){          d &lt;- data[indices,]         # PS for 1978     ps1 &lt;- glm(trt ~ age + I(age^2) + educ + I(educ^2) + black + hisp +                             re74 + I(re74^2) + re75 + I(re75^2) + nodeg,                        data=d, family = &quot;binomial&quot;)          # weights     w1 &lt;- 1/ifelse(d$trt==1,predict(ps1,type=&quot;response&quot;),                    1-predict(ps1,type=&quot;response&quot;))          # Marginal structural model     msm1 &lt;- lm(re78 ~ trt, weights=w1, data=d)          # PS for 1975 (same as above but doesnt have 1975 earnings)     ps2 &lt;- glm(trt ~ age + I(age^2) + educ + I(educ^2) + black + hisp +                             re74 + I(re74^2) + nodeg,                        data=d, family = &quot;binomial&quot;)          w2 &lt;- 1/ifelse(d$trt==1,predict(ps2,type=&quot;response&quot;),                    1-predict(ps2,type=&quot;response&quot;))          msm2 &lt;- lm(re75 ~ trt, weights=w2, data=d)          # return the paramter of interest     return(coef(msm1)[2]-coef(msm2)[2]) }   results &lt;- boot(data = nsw74demo,                  statistic = did, R = 1000,                  strata = factor(nsw74demo$trt)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred  ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred results ##  ## STRATIFIED BOOTSTRAP ##  ##  ## Call: ## boot(data = nsw74demo, statistic = did, R = 1000, strata = factor(nsw74demo$trt)) ##  ##  ## Bootstrap Statistics : ##     original   bias    std. error ## t1* 1322.253 4.992589    735.5867 plot(results)  boot.ci(results, type = c(&quot;norm&quot;,&quot;basic&quot;, &quot;perc&quot;) ) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ##  ## CALL :  ## boot.ci(boot.out = results, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;)) ##  ## Intervals :  ## Level      Normal              Basic              Percentile      ## 95%   (-124, 2759 )   (-191, 2743 )   ( -98, 2835 )   ## Calculations and Intervals on Original Scale     "],
["references.html", "Chapter 4 References", " Chapter 4 References    Austin, Peter C. 2011. “An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.” Multivariate Behavioral Research 46 (3). Taylor &amp; Francis: 399–424.   Canty, Angelo, and Brian Ripley. 2016. Boot: Bootstrap Functions (Originally by Angelo Canty for S). https://CRAN.R-project.org/package=boot.   dAgostino, Ralph B. 1998. “Tutorial in Biostatistics: Propensity Score Methods for Bias Reduction in the Comparison of a Treatment to a Non-Randomized Control Group.” Stat Med 17 (19): 2265–81.   Davidian, Marie. 2005. Simulation Studies in Statistics. http://www4.stat.ncsu.edu/~davidian/st810a/simulation_handout.pdf.   Davison, Anthony Christopher, and David Victor Hinkley. 1997. Bootstrap Methods and Their Application. Vol. 1. Cambridge university press.   Efron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.   Friedman, Jerome, Trevor Hastie, Noah Simon, and Rob Tibshirani. 2016. Glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. https://CRAN.R-project.org/package=glmnet.   Hallgren, Kevin A. 2013. “Conducting Simulation Studies in the R Programming Environment.” Tutorials in Quantitative Methods for Psychology 9 (2). NIH Public Access: 43.   Hastie, Tibshirani, and Friedman. 2009. The Elements of Statistical Learning (2nd Edition). Vol. 2. Cambridge university press. http://statweb.stanford.edu/~tibs/ElemStatLearn/.   Joseph, Lawrence. 2010. Principles of Inferential Statistics in Medicine. http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/EPIB-607/notes.pdf.   Maindonald, John H., and W. John Braun. 2015. DAAG: Data Analysis and Graphics Data and Functions. https://CRAN.R-project.org/package=DAAG.   Rosenbaum, Paul R, and Donald B Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika 70 (1). Biometrika Trust: 41–55.   Stephens, David A. 2015. “MATH 680 - Computation Intensive Statistics Course Notes.” Notes. McGill University. http://www.math.mcgill.ca/dstephens/.   Sveidqvist, Knut, Mike Bostock, Chris Pettitt, Mike Daines, Andrei Kashcha, and Richard Iannone. 2016. DiagrammeR: Create Graph Diagrams and Flowcharts Using R. https://CRAN.R-project.org/package=DiagrammeR.   Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological). JSTOR, 267–88.   Wang, Yishu, Orla Murphy, Maxime Turgeon, ZhuoYu Wang, Sahir R Bhatnagar, Juliana Schulz, and Erica EM Moodie. 2015. “The Perils of Quasi-Likelihood Information Criteria.” Stat 4 (1). Wiley Online Library: 246–54.   Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/.   ———. 2016. Bookdown: Authoring Books with R Markdown. https://github.com/rstudio/bookdown.       All flowcharts are created using the DiagrammeR pacakge (Sveidqvist et al. 2016)↩ This example has been reproduced with permission from Dr. David A. Stephens (Stephens 2015)↩  "]
]
